**What is expected for an ML Report?** 

 - Model Description
    - What model are you using, and why was it selected?
    - Did you implement the model yourself or use an open-source implementation?
    - What library or framework was used (e.g., TensorFlow, PyTorch, scikit-learn)?

- Dataset & Task
    - What training data did you use?
    - Is the dataset labeled? Is the task supervised, unsupervised, or semi-supervised?
    - How was the data preprocessed (e.g., cleaning, normalization, tokenization, augmentation)?

- Training Procedure
    - How were the train, validation, and test sets separated?
    - What hyperparameters were used, and how were they tuned?
    - Which optimizer and loss function were used?
    - How was the model trained (your personal computer, Google Colab, Compute Canada, etc.)?

- Evaluation & Verification
    - What evaluation metrics were used (accuracy, F1, MSE, etc.)?
    - Verification steps and results.

- Results & Analysis
    - Critical analysis of the results.
    - Discussion of overfitting/underfitting, limitations, and failure cases.
    - What could be improved with more time or more data?

- Ethical or Data Considerations (if applicable)
    - Issues related to bias, privacy, or fairness in the dataset or predictions.

**Note:** Not all of these factors may apply to your specific project, but you are expected to thoroughly address all elements that are relevant to your approach. You must discuss anything that has relevance, even in a minor way. If you intentionally excluded a component, you must clearly explain why it was excluded.